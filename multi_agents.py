# multi_agent.py - Syst√®me Multi-Agents avec LangChain
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain.schema import HumanMessage, SystemMessage, AIMessage
from langchain_community.callbacks.manager import get_openai_callback
from typing import Dict, List, Any, Tuple
import json
import re
from datetime import datetime
from pydantic import BaseModel

class MultiAgentYouTubeAssistant:
    def __init__(self, api_key: str, model_name: str = "gpt-4"):
        """
        Initialise le syst√®me multi-agents
        
        Args:
            api_key: Cl√© API OpenAI
            model_name: Mod√®le √† utiliser (gpt-4, gpt-3.5-turbo, etc.)
        """
        self.api_key = api_key
        self.llm = ChatOpenAI(
            openai_api_key=api_key,
            model_name=model_name,
            temperature=0.1,  # Faible temp√©rature pour l'agent analyseur
            max_tokens=2000
        )
        
        # LLM pour l'agent r√©pondeur (plus cr√©atif)
        self.response_llm = ChatOpenAI(
            openai_api_key=api_key,
            model_name=model_name,
            temperature=0.7,
            max_tokens=1500
        )
        
        self.setup_agents()
    
    def setup_agents(self):
        """Configure les prompts des deux agents"""
        
        # Agent 1: Analyseur de questions
        self.analyzer_prompt = ChatPromptTemplate.from_messages([
            SystemMessagePromptTemplate.from_template("""
Tu es un expert en analyse de questions sur du contenu vid√©o YouTube.
Ton r√¥le est d'analyser pr√©cis√©ment la question de l'utilisateur et de d√©terminer:

1. **TYPE DE QUESTION** (choisis UN seul type):
   - "definition": L'utilisateur demande une d√©finition ou explication d'un concept
   - "clarification": L'utilisateur veut clarifier quelque chose qui vient d'√™tre dit
   - "context": L'utilisateur veut du contexte ou des d√©tails suppl√©mentaires
   - "summary": L'utilisateur veut un r√©sum√© de ce qui a √©t√© dit
   - "timestamp": L'utilisateur fait r√©f√©rence √† un moment sp√©cifique
   - "comparison": L'utilisateur veut comparer des √©l√©ments
   - "application": L'utilisateur veut savoir comment appliquer quelque chose
   - "general": Question g√©n√©rale sur le sujet de la vid√©o

2. **STRAT√âGIE DE CONTEXTE** (choisis UNE strat√©gie):
   - "current_focus": Se concentrer principalement sur le moment actuel
   - "recent_context": Utiliser les 2-3 derni√®res minutes
   - "broad_context": Chercher dans toute la vid√©o
   - "specific_search": Chercher des mots-cl√©s sp√©cifiques

3. **STYLE DE R√âPONSE** (choisis UN style):
   - "concise": R√©ponse courte et directe
   - "detailed": Explication d√©taill√©e avec exemples
   - "step_by_step": Explication √©tape par √©tape
   - "conversational": Ton naturel et accessible

4. **MOTS-CL√âS IMPORTANTS**: Extrais les termes cl√©s de la question

R√©ponds UNIQUEMENT en format JSON valide:
{
    "question_type": "...",
    "context_strategy": "...",
    "response_style": "...",
    "keywords": ["mot1", "mot2", ...],
    "confidence": 0.95,
    "reasoning": "Courte explication de ton analyse"
}
"""),
            HumanMessagePromptTemplate.from_template("""
QUESTION DE L'UTILISATEUR: "{user_question}"

MOMENT ACTUEL DANS LA VID√âO: {current_time_formatted}

CONTEXTE AUTOUR DU MOMENT ACTUEL:
{priority_context_preview}

Analyse cette question et r√©ponds en JSON:
""")
        ])
        
        # Agent 2: G√©n√©rateur de r√©ponses
        self.responder_prompt = ChatPromptTemplate.from_messages([
            SystemMessagePromptTemplate.from_template("""
Tu es un assistant IA expert en explication de contenu vid√©o YouTube.
Tu re√ßois une analyse d√©taill√©e de la question utilisateur et tu dois g√©n√©rer la meilleure r√©ponse possible.

DIRECTIVES SELON LE TYPE DE QUESTION:

**DEFINITION**: Donne une d√©finition claire, puis explique dans le contexte de la vid√©o
**CLARIFICATION**: √âclaircis le point confus en te basant sur ce qui vient d'√™tre dit
**CONTEXT**: Fournis le contexte manquant, connecte avec d'autres parties de la vid√©o
**SUMMARY**: R√©sume de fa√ßon structur√©e et claire
**TIMESTAMP**: Fais r√©f√©rence aux moments pr√©cis avec les timestamps
**COMPARISON**: Compare les √©l√©ments en soulignant les diff√©rences/similitudes
**APPLICATION**: Donne des exemples concrets d'application
**GENERAL**: R√©ponds de fa√ßon g√©n√©rale mais en restant ancr√© dans le contenu vid√©o

DIRECTIVES SELON LE STYLE:

**CONCISE**: R√©ponse en 2-3 phrases maximum
**DETAILED**: Explication compl√®te avec exemples du contexte vid√©o
**STEP_BY_STEP**: Structure en √©tapes num√©rot√©es ou √† puces
**CONVERSATIONAL**: Ton naturel, comme si tu expliquais √† un ami

R√àGLES IMPORTANTES:
- Utilise les timestamps [MM:SS] quand tu fais r√©f√©rence √† d'autres moments
- Reste fid√®le au contenu de la vid√©o
- Si l'info n'est pas dans le contexte fourni, dis-le clairement
- Sois pr√©cis et √©vite les g√©n√©ralit√©s
"""),
            HumanMessagePromptTemplate.from_template("""
QUESTION ORIGINALE: "{original_question}"

ANALYSE DE LA QUESTION:
- Type: {question_type}
- Strat√©gie de contexte: {context_strategy}  
- Style de r√©ponse: {response_style}
- Mots-cl√©s: {keywords}

MOMENT ACTUEL: {current_time_formatted}

CONTEXTE PRIORITAIRE (autour du moment actuel):
{priority_context}

CONTEXTE DE R√âF√âRENCE (reste de la vid√©o):
{extended_context}

G√©n√®re maintenant une r√©ponse optimale selon l'analyse fournie:
""")
        ])
    
    def analyze_question(self, user_question: str, contextual_data: Dict) -> Dict:
        """
        Agent 1: Analyse la question de l'utilisateur
        """
        try:
            # Cr√©er un aper√ßu du contexte prioritaire pour l'analyseur
            priority_preview = (
                contextual_data['priority_window_text'][:300] + "..."
                if len(contextual_data['priority_window_text']) > 300
                else contextual_data['priority_window_text']
            )

            # Formatage du prompt pour l'analyseur
            analyzer_messages = self.analyzer_prompt.format_messages(
                user_question=user_question,
                current_time_formatted=contextual_data['current_time_formatted'],
                priority_context_preview=priority_preview
            )

            # Appel √† l'agent analyseur avec invoke()
            with get_openai_callback() as cb:
                analysis_response = self.llm.invoke(analyzer_messages)
                print(f"üí∞ Co√ªt Agent Analyseur: ${cb.total_cost:.4f}")

            # R√©cup√©rer le texte brut
            analysis_text = analysis_response.content.strip()

            # Essayer d'extraire du JSON
            json_match = re.search(r'\{.*\}', analysis_text, re.DOTALL)
            if json_match:
                candidate = json_match.group(0)
            else:
                # Si pas d'accolades, on essaie de forcer
                candidate = "{" + analysis_text + "}"

            # Nettoyage basique
            candidate = candidate.replace("'", '"').replace("\n", " ").strip()

            try:
                analysis_json = json.loads(candidate)
            except json.JSONDecodeError as e:
                print(f"‚ö†Ô∏è JSON mal form√© m√™me apr√®s nettoyage ({e}), fallback utilis√©")
                analysis_json = {
                    "question_type": "general",
                    "context_strategy": "current_focus",
                    "response_style": "conversational",
                    "keywords": [],
                    "confidence": 0.4,
                    "reasoning": f"Parsing failed: {str(e)}"
                }

            print(f"üîç Analyse termin√©e: "
                f"{analysis_json.get('question_type', 'general')} | "
                f"{analysis_json.get('context_strategy', 'current_focus')} | "
                f"{analysis_json.get('response_style', 'conversational')}")
            return analysis_json

        except Exception as e:
            print(f"‚ùå Erreur dans analyze_question: {e}")
            # Analyse par d√©faut
            return {
                "question_type": "general",
                "context_strategy": "current_focus",
                "response_style": "conversational",
                "keywords": [],
                "confidence": 0.3,
                "reasoning": f"Error fallback: {str(e)}"
            }

    def generate_response(self, original_question: str, analysis: Dict, contextual_data: Dict) -> str:
        """
        Agent 2: G√©n√®re la r√©ponse bas√©e sur l'analyse
        """
        try:
            # Ajuster le contexte selon la strat√©gie analys√©e
            context_data = self.adjust_context_by_strategy(contextual_data, analysis)
            
            # Formatage du prompt pour le g√©n√©rateur de r√©ponses
            responder_messages = self.responder_prompt.format_messages(
                original_question=original_question,
                question_type=analysis['question_type'],
                context_strategy=analysis['context_strategy'],
                response_style=analysis['response_style'],
                keywords=', '.join(analysis.get('keywords', [])),
                current_time_formatted=contextual_data['current_time_formatted'],
                priority_context=context_data['priority_context'],
                extended_context=context_data['extended_context']
            )
            
            # Appel √† l'agent r√©pondeur
            with get_openai_callback() as cb:
                response = self.llm.invoke(responder_messages)
                print(f"üí∞ Co√ªt Agent R√©pondeur: ${cb.total_cost:.4f}")
            
            return response.content.strip()
            
        except Exception as e:
            print(f"‚ùå Erreur dans generate_response: {e}")
            return f"D√©sol√©, une erreur est survenue lors de la g√©n√©ration de la r√©ponse: {str(e)}"
    
    def adjust_context_by_strategy(self, contextual_data: Dict, analysis: Dict) -> Dict:
        """
        Ajuste le contexte fourni selon la strat√©gie d√©termin√©e par l'analyseur
        """
        strategy = analysis['context_strategy']
        
        if strategy == "current_focus":
            # Se concentrer sur le moment actuel seulement
            return {
                'priority_context': contextual_data['priority_window_text'],
                'extended_context': contextual_data['extended_context_summary'][:500] + "..."
            }
            
        elif strategy == "recent_context":
            # Contexte r√©cent plus large
            return {
                'priority_context': contextual_data['priority_window_text'],
                'extended_context': contextual_data['extended_context_summary'][:800] + "..."
            }
            
        elif strategy == "broad_context":
            # Utiliser tout le contexte disponible
            return {
                'priority_context': contextual_data['priority_window_text'],
                'extended_context': contextual_data['extended_context_summary']
            }
            
        elif strategy == "specific_search":
            # Rechercher des mots-cl√©s sp√©cifiques (impl√©mentation simplifi√©e)
            keywords = analysis.get('keywords', [])
            filtered_context = self.filter_context_by_keywords(
                contextual_data['extended_context_summary'], 
                keywords
            )
            return {
                'priority_context': contextual_data['priority_window_text'],
                'extended_context': filtered_context
            }
        
        # D√©faut
        return {
            'priority_context': contextual_data['priority_window_text'],
            'extended_context': contextual_data['extended_context_summary']
        }
    
    def filter_context_by_keywords(self, context: str, keywords: List[str]) -> str:
        """
        Filtre le contexte pour ne garder que les sections contenant les mots-cl√©s
        """
        if not keywords:
            return context
        
        # Diviser le contexte en paragraphes
        paragraphs = context.split('\n\n')
        relevant_paragraphs = []
        
        for paragraph in paragraphs:
            # V√©rifier si le paragraphe contient au moins un mot-cl√©
            paragraph_lower = paragraph.lower()
            if any(keyword.lower() in paragraph_lower for keyword in keywords):
                relevant_paragraphs.append(paragraph)
        
        return '\n\n'.join(relevant_paragraphs) if relevant_paragraphs else context[:1000] + "..."
    
    def process_question(self, user_question: str, contextual_data: Dict) -> Dict:
        """
        Pipeline complet: Analyse + G√©n√©ration de r√©ponse
        
        Returns:
            Dict contenant la r√©ponse et les m√©tadonn√©es de l'analyse
        """
        print(f"üöÄ D√©but du traitement multi-agents")
        print(f"üìù Question: {user_question}")
        print(f"‚è∞ Moment: {contextual_data['current_time_formatted']}")
        
        # √âtape 1: Analyse de la question
        analysis = self.analyze_question(user_question, contextual_data)
        
        # √âtape 2: G√©n√©ration de la r√©ponse
        response = self.generate_response(user_question, analysis, contextual_data)
        
        # Retourner les r√©sultats complets
        return {
            'response': response,
            'analysis': analysis,
            'timestamp': datetime.now().isoformat(),
            'context_used': len(contextual_data['priority_context'])
        }

# Fonction utilitaire pour tester le syst√®me
def test_multi_agent_system():
    """
    Test rapide du syst√®me multi-agents
    """
    # Configuration de test
    import os
    from dotenv import load_dotenv
    
    load_dotenv()
    api_key = os.getenv('OPENAI_API_KEY')
    
    if not api_key:
        print("‚ùå OPENAI_API_KEY non trouv√©e dans .env")
        return
    
    # Initialiser le syst√®me
    assistant = MultiAgentYouTubeAssistant(api_key)
    
    # Donn√©es de test
    test_contextual_data = {
        'current_time_formatted': '15:30',
        'priority_window_text': '[15:20] Nous allons maintenant parler des algorithmes de tri. [15:25] Le tri √† bulles est l\'un des plus simples √† comprendre. [15:30] Il compare chaque √©l√©ment avec son voisin.',
        'extended_context_summary': '[10:00] Introduction aux algorithmes...\n[12:00] Les structures de donn√©es...',
        'priority_context': []
    }
    
    # Test
    result = assistant.process_question(
        "Qu'est-ce que le tri √† bulles ?", 
        test_contextual_data
    )
    
    print(f"\n‚úÖ R√©sultat du test:")
    print(f"Type de question: {result['analysis']['question_type']}")
    print(f"Strat√©gie: {result['analysis']['context_strategy']}")
    print(f"Style: {result['analysis']['response_style']}")
    print(f"R√©ponse: {result['response']}")

if __name__ == "__main__":
    test_multi_agent_system()